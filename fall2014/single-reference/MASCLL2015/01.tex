\begin{block}{Paucity of References}
\begin{description}
\item[What is the problem?] 
\begin{itemize}
\item Machine Translation Evaluation
\item 
Paucity of references
\item Credit for translations with equivalent meanings
\end{itemize}
\item[what is the cause? ]
The Paucity of References Problem
sentences  have billions of  translations into one foreign language~\cite{dreyer-marcu:2012:NAACL-HLT}. 
By a correct translation of a source sentence we mean any sentence in the target language that has the same meaning as the source sentence. 
BLEU designed assuming   system output would be compared to   four reference translations 
In practice   one  reference translation 
assume only one human generated reference is available
a perfect  translation  can get a very low BLEU score 
BLEU incorporates features that capture variations of references 
Bleu does not require an exact match for the entire sentence 
allocates credit to sub-sentence segments by considering matches  of overlapping $n$-grams for   n equals 1 through 4
Even with this partial credit assignment and assuming four references,  this coverage is not enough to be representative of billions of possible correct translations

Improving BLEU

methods of generating synonyms would improve  BLEU 
Like our present project, several previous projects have been devoted to developing metrics that extend BLEU by expanding the set of references to include more sentences with equivalent meaning. 
Some of these projects like METEOR\cite{banerjee-lavie:2005:MTSumm}\cite{denkowski:lavie:meteor-wmt:2014}, TERP\cite{snover-06}\cite{snover-08} and HyTER have produced popular metrics. 


We list some features we would like to see in an improved BLEU metric and some features we want to keep.  
\begin{itemize}
\item 
We want to assign credit to approximate matches. 
System output that expresses similar meaning to the input should receive adequate credit from the metric. 
\item 
We want  it to adapt easily to a new language pair. 
That is, we want our metric to require little or no human labor for annotation, and little or no NLP resources. 

\item 
We want a metric that assigns reasonable scores to partial output. 
We  will use this metric as a component that measures translation quality in interactive incremental language systems. 
Such systems need to measure the quality of partial inputs. 
\end{itemize}

Word Similarities

The ultimate goal of our project is to attack the paucity of references problem directly by using a semantic vector space to generate references equivalent to a single given reference
In this paper however, we investigate an easier solution. 
We use the similarity measure defined in a semantic vector space generated by the tool word2vec to provide scores for $n$grams that do not have exact matches in the reference. 
Word2vec embeds single word tokens into a vector space
similarity function can be extended from words to phrases by averaging vectors
build two enhanced BLEU metrics that compute similarities between phrases 
One of these metrics yields a significant boost in scores relative to BLEU 
Many $n$grams that did not match exactly receive a non-zero similarity measure with reference phrases 
large portion of matches are spurious 

We conclude that for a metric intended to evaluate at a sentence or corpus level, it is not appropriate to use the similarity measure to provide approximate match scores for phrases 
We instead believe that more sophisticated approaches need to be taken to generate phrases with equivalent meaning to a given reference
\end{description}
\end{block}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "poster"
%%% End: 

