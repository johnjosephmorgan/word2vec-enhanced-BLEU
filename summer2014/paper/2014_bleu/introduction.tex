\section{Introduction}
\label{sec:intro}

Many Army operations involve a need to translate English text into a
foreign language. For example in the case where Army special forces
train foreign army units the texts used in these training operations
should be available in the language of the foreign army.

Depending on its direction, translation can have two purposes for a
country: assimilation, from a foreign language into the country's
native language, or dissemenation, from the country's native language
into a foreign language. Assimilation is highly valued in the
Intelligence Community where analysts want to know what is being
written in foreign texts. Dissemenation of English into low resourced
foreign languages does not receive the attention given to
assimilation. We are interested in using SMT for
dissemenation purposes in Army training operations. Development of
evaluation metrics is an aspect of dissemination that would benefit
from more attention.

The traditional BLEU\cite{BLEU} metric uses exact string matching to score smt
decoder output with reference transcriptions. This makes BLEU
language-independent and thus is equally benefitial for assimilation
and dissemenation purposes. BLEU is well known to have
weaknesses\cite{Callison-Burch2006EACL}. Work has been done to remedy these weaknesses by
incorporating llinguistic features into the evaluation metric. These
enhancements make the metrics more accurate and correlate more with
human judgements.

Meteor\cite{banerjee-lavie2005MTSumm} is an evaluation metric that takes into consideration stem, synonym, and paraphrase matches between words inaddition to exact string matches. Evaluation metrics like meteor exist for resource-rich languages \footnote{Although see: \url{http://www.cs.cmu.edu/~mdenkows/meteor-universal.html} for an interesting development that makes meteor available in other languages. Note that this tool  requires a parallel corpus large enough to train a moses smt system.}, but rare for low-resourced languages of interest to the Army. For dissemmenation purposes, We would like to have an evaluation metric similar to Meteor that does not depend on high powered nlp tools like parsers or pos taggers which are not readily available in most languages.

Meteor extends BLEU by incorporating semantic features into the evaluation of SMT output. We similarly enhance BLEU with semantic knowledge by replacing words in the reference transcriptions with words that are close in meaning. 

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../2014_bleu"
%%% End: 
